{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562611f3",
   "metadata": {},
   "source": [
    "# Machine Learning in Julia - MLJ.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9aaf79",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ba19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using CSV, DataFrames\n",
    "using Random\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb614c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = CSV.read(\"bank-additional-full.csv\", DataFrame, delim=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764514cd",
   "metadata": {},
   "source": [
    "`MLJ` use the notion of the **scientific types** beside the data types. Scientific types specify how the features should be interpreted by the ML models and the users - the basic types are:\n",
    "* `Continuous`,\n",
    "* `Count`,\n",
    "* `Multiclass{N}`, \n",
    "* `OrderedFactor{N}`, \n",
    "* `Textual`.\n",
    "\n",
    "More information about the scientific types can be found in the [MLJ documentation](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/#Data-containers-and-scientific-types). The `schema` function will produce a data type and scientific type for each column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf930661",
   "metadata": {},
   "source": [
    "The default mapping between the data and scientific types didn't produce the desired outcome. Let's use `coerce` function to change all `Textual` types to `Multiclass` and `age` feature to the `Continuous` type. The binary target variable `y` is also coerced to `OrderedFactor` which will indicate `yes` value to be intepreted as positive class during classification. Note that after the conversion data types changed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbe9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = coerce(df, :age => Continuous, Textual => Multiclass);\n",
    "df = coerce(df, :y => OrderedFactor)\n",
    "schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels!(df.y, [\"no\",\"yes\"]) # setting the ordering of the `y` column\n",
    "levels(df.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e499815",
   "metadata": {},
   "source": [
    "## 3. Data preprocsessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9b118",
   "metadata": {},
   "source": [
    "After modifying the schema, let's proceed with further preprocessing. `unpack` function splits the dataset into independent variables (`X`) and target variable (`y`). Features can also be removed in the same step - let's remove the `duration` variable as it's highly related to the target and recommended for removal by the dataset owner (for details check the `bank-additional-names.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(groupby(df, :y), :duration .=> [minimum median mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83050e06",
   "metadata": {},
   "source": [
    "Statistics calculated above confirm close relation between target variable and duration of the call - mean `duration` is significantly higher, when customer subscribed to the term deposit. Let's proceed with splitting the dataset and removal of the `duration` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(df, ==(:y), !=(:duration));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798c61f",
   "metadata": {},
   "source": [
    "After the split, `y` is a `CategoricalArray` with two levels: \"no\" and \"yes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63c650",
   "metadata": {},
   "source": [
    "`X` remains a `DataFrame` with `duration` and `y` columns gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb133f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f30fd9",
   "metadata": {},
   "source": [
    "In most cases data needs to be preprocessed to be properly used for ML models training. `MLJ` has multiple transformers to help with common data preparation tasks, in particular:\n",
    "* standardizing the numeric features,\n",
    "* one-hot encoding the nominal features,\n",
    "* transforming skewed numeric features,\n",
    "* imputing the missing values.\n",
    "\n",
    "Let's use `Standardizer` and `OneHotEncoder` transformers on the Bank Marketing data - both [standardization](https://en.wikipedia.org/wiki/Standard_score) and [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics) are common preprocessing steps. By default `Standardizer` standardize only features with `Continuous` scientific type. Similarly, `OneHotEncoder` transforms only `Multiclass` and `OrderedFactor` types. Multiple preparation steps can be combined in **pipelines** by using the `Pipeline` constructor or `|>` symbol - pipelines make repetitive tasks simpler and easier to execute.\n",
    "\n",
    "`MLJ` expose a common interface for managing and manipulating transformers, models and other objects. To run the preprocessing pipeline we'll utilize:\n",
    "* `machine` function which binds an algorithm (model, transformer, pipelines, etc.) to the data,\n",
    "* `fit!` function for training the algorithm,\n",
    "* `transform` function to use trained pipepline to get the standardized and one-hot encoded dataset.\n",
    "\n",
    "The cell below will show a transformed `DataFrame` - look how it differs from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809e74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "?Standardizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe = Standardizer() |> OneHotEncoder()\n",
    "preproc_wrapped = machine(preproc_pipe, X)\n",
    "X_prepared = MLJ.transform(fit!(preproc_wrapped), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63eaf6f",
   "metadata": {},
   "source": [
    "Using another element from the common interface - `fitted_params` function - we can inspect the parameters learned by the `Standardizer` while running `fit!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params(preproc_wrapped).standardizer.features_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2be744",
   "metadata": {},
   "source": [
    "## 4. MLJ as repository of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec4eb3",
   "metadata": {},
   "source": [
    "`MLJ` framework also works as a repository and wrapper of ML models built in various Julia packages. Using `models` function, let's check which models will be suited to the posed classification task considering types of predictors and target variable. Additionally, let's find only models fully implemented in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51f3bb2",
   "metadata": {},
   "source": [
    "Before a particular model will be used in the `MLJ` workflow, it must be available in the environment. `load_path` function returns a package need for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = m -> (matching(m, X_prepared, y) && m.is_pure_julia)\n",
    "models(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c784b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (model, pkg) in [(\"RandomForestClassifier\", \"DecisionTree\"),\n",
    "                     (\"DecisionTreeClassifier\", \"DecisionTree\")]\n",
    "println(load_path(model, pkg=pkg))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72093377",
   "metadata": {},
   "source": [
    "We can load the models with the `@load` macro. If the model name appears in more than one package, the package name must be explicitly stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Forest = @load RandomForestClassifier pkg=\"DecisionTree\"\n",
    "Tree = @load DecisionTreeClassifier pkg=\"DecisionTree\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d91a8",
   "metadata": {},
   "source": [
    "## 5. Training and evaluating the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4cf708",
   "metadata": {},
   "source": [
    "We are ready to train the loaded models - let's streamline the training and evaluation process with `MLJ`. The models with the default hyperparameter values are wrapped in the `evaluate` function which will do the following:\n",
    "* split the provided data according to the resampling strategy - in the case below the `Holdout` specifies the data split into train/test subsets with 80/20 ratio and random seed equal to 42,\n",
    "* fit the model on the training data and evaluate it on the test data calculating each measure specified in the `measure` parameter,\n",
    "* return the evaluation report with relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e236278",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(groupby(df, :y), nrow=>:count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c732c49",
   "metadata": {},
   "source": [
    "How the training and evaluation steps are done step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151c708",
   "metadata": {},
   "source": [
    "1. Split the data into the training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test), (y_train, y_test) = partition((X_prepared,y), 0.8, rng=42, multi=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c66c9",
   "metadata": {},
   "source": [
    "2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(Forest(), X_train, y_train)\n",
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f23f17",
   "metadata": {},
   "source": [
    "3. Make a prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = predict_mode(mach, X_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893539b2",
   "metadata": {},
   "source": [
    "4. Evaluate or assess the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(ŷ, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe9143",
   "metadata": {},
   "source": [
    "The same steps can be performed with `evaluate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = [accuracy, auc, f1score]\n",
    "split = Holdout(fraction_train=0.8, rng=42)\n",
    "rng_mt = MersenneTwister(42)\n",
    "for m in [Forest, Tree]\n",
    "    eval_report = evaluate(m(rng=rng_mt), X_prepared, y, resampling=split, measure=measures)\n",
    "    println(m)\n",
    "    println.(eval_report.measure, \": \", round.(eval_report.measurement, digits=3))\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda8f02",
   "metadata": {},
   "source": [
    "To list all available metrics, use `measures` function similar to the `models`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218eddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.measures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0362a",
   "metadata": {},
   "source": [
    "Three ML models with the default hyperparameters values were built and evaluated. Out of the trained models `RandomForestClassifier` achieved the best AUC and F1-score. Let's tune the hyperparameters of the Random Forest  based on the AUC metric. \n",
    "\n",
    "The list of available hyperparemeters and their default values is displayed when the instance of the Random Forest is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = Forest(rng=rng_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87feee",
   "metadata": {},
   "source": [
    "To learn more about each hyperparameter review documentation for `RandomForestClassifier` by calling `?Forest()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919b889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "?Forest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cc0ae",
   "metadata": {},
   "source": [
    "## 6. Cross-validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d034d24",
   "metadata": {},
   "source": [
    "Tuning all hyperparameters of `RandomForestClassifier` would be computionally demanding. To finish within a reasonable time, we'll tune two arbitrary picked hyperparameters: `max_depth` and `n_trees`. `max_depth` limits the growth of each tree in the ensemble - deep trees may be overfitted, while shallow trees may be biased. `n_trees` determines number of trees in the ensemble - the predictive power of the forest should initially increase with the number of trees and then saturate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171167cc",
   "metadata": {},
   "source": [
    "Tuning in `MLJ` can be performed easily with `TunedModel` interface. The tuning specification includes:\n",
    "* `model` - the model to be tuned, in our case an instance of `RandomForestClassifier`,\n",
    "* `resampling` - mechanism for splitting the data into training and validation subsets, e.g. cross-validation (below 3-fold cross-validation specified in `CV` resampling strategy),\n",
    "* `tuning` - tuning strategy for searching the hyperparameters space, in the example below `Grid(resolution=4)` constructs grid search with cartesian product of six evenly spaced values for each tuned hyperparameter (16 models for two tuned hyperparameters),\n",
    "* `range` - specification of tuned hyperparameters including hyperparameter names and their extreme values,\n",
    "* `measure` - metric used for models evaluation - as specfied above, AUC is used.\n",
    "\n",
    "Please note the tuning is computationally intensive excercise and may take few minutes to complete. The progress bar will indicate how many models are already evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_range = [range(forest, :n_trees, lower=10, upper=80),\n",
    "                    range(forest, :max_depth, lower=2, upper=30)]\n",
    "self_tuning_forest = TunedModel(\n",
    "    model = forest,\n",
    "    resampling = CV(nfolds=3, rng=rng_mt),\n",
    "    tuning = Grid(resolution=4),\n",
    "    range = hyperparam_range,\n",
    "    measure = auc)\n",
    "mach = machine(self_tuning_forest, X_prepared, y)\n",
    "fit!(mach, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcfedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35713e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = fitted_params(mach).best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b44c8",
   "metadata": {},
   "source": [
    "Using initial `evaluate` call we can easily compare tuned model performance to the default forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_model, X_prepared, y, resampling=split, measure=measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mode(mach, X_prepared[1:10,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
